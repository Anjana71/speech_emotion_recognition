ğŸŒŸ âœ¨ Real-Time Speech Emotion Recognition (SER)

A Hybrid Deep Learning + Whisper ASR System

<p align="center"> <img src="Ser.png" alt="SER Banner" width="80%"> </p>

```
ğŸ™ï¸ Overview
This project implements a Real-Time Speech Emotion Recognition (SER) system capable of detecting human emotions directly from speech.
It integrates:

ğŸ§ Audio feature extraction
ğŸ§  Hybrid LSTMâ€“GRU deep learning model
ğŸ—£ï¸ OpenAI Whisper ASR for speech-to-text
ğŸŒ Streamlit for a clean, interactive UI
The system works with live microphone input or uploaded audio files and supports real-time emotion visualization.

ğŸš€ Key Features
ğŸ¤ Live microphone-based emotion detection
ğŸ“‚ Emotion prediction from uploaded audio
ğŸ§  LSTM + GRU + Attention hybrid architecture
ğŸ—£ï¸ Whisper ASR for transcription
ğŸ§­ Emotion segmentation/timeline visualization
ğŸŒ Streamlit web interface
ğŸ”Š Support for multiple datasets, modes, and speakers

ğŸ­ Emotion Classes
Neutral â€¢ Happy â€¢ Sad â€¢ Angry â€¢ Fear â€¢ Disgust â€¢ Surprise â€¢ Boredom â€¢ Excited

```
<p align="center"> <img src="images/emotion_wheel.png" width="45%"> </p>
ğŸ“‚ Sample Output Screenshots
ğŸ”¹ Main UI
<p align="center"> <img src="UI.png" width="80%" alt="UI Main"> </p>
ğŸ”¹ Uploading file
<p align="center"> <img src="uploading audio file.png" width="80%" alt="Emotion Output"> </p>
ğŸ”¹ Output Loading
<p align="center"> <img src="Loading.png" width="80%" alt="Emotion Output"> </p>
ğŸ”¹ Emotion Prediction Output
<p align="center"> <img src="Output.png" width="80%" alt="Emotion Output"> </p>
ğŸ”¹ Emotion Prediction Output
<p align="center"> <img src="Output1.png" width="80%" alt="Emotion Output"> </p>
ğŸ”¹ Emotion Prediction Output
<p align="center"> <img src="Output of video file.png" width="80%" alt="Emotion Output"> </p>
ğŸ”¹ Transcription Using Whisper ASR
<p align="center"> <img src="Emotion with transcription.png" width="80%" alt="Whisper Transcription"> </p>




## ğŸ§© Project Structure

```
ğŸ“¦ speech-emotion-recognition
â”‚
â”œâ”€â”€ data/                     # Raw datasets & extracted features
â”œâ”€â”€ models/                   # Trained .h5 model files
â”œâ”€â”€ Feature_extraction.py     # Audio preprocessing & feature extraction
â”œâ”€â”€ lstm_gru_model.py         # LSTM + GRU + Attention model
â”œâ”€â”€ Rebuild_model.py          # Load & rebuild trained model
â”œâ”€â”€ frontend.py               # Streamlit frontend
â”œâ”€â”€ utils/                    # Helper functions (segmentation, metrics, preprocessing)
â”œâ”€â”€ images/                   # UI, prediction & output images
â”œâ”€â”€ requirements.txt          # Dependencies
â””â”€â”€ README.md                 # Documentation
```

```
ğŸµ Datasets Used
All audio converted to 16kHz mono WAV.
RAVDESS
CREMA-D
TESS
SAVEE
EMO-DB
Using multiple datasets improves robustness across speakers, accents, and recording conditions.

ğŸ› ï¸ Installation
1ï¸âƒ£ Clone the Repository
git clone https://github.com/yourusername/speech-emotion-recognition.git
cd speech-emotion-recognition

2ï¸âƒ£ Install Dependencies
pip install -r requirements.txt

3ï¸âƒ£ Install FFmpeg
Required for audio conversion and Whisper ASR.

â–¶ï¸ How to Run
Start the Streamlit app:
streamlit run frontend.py
Visit http://localhost:8501
 in your browser.

You can now:
ğŸ™ï¸ Record live audio
ğŸ“‚ Upload .wav, .mp3, .ogg, etc.
ğŸ§  View real-time emotion prediction
ğŸ—£ï¸ See Whisper-generated transcript
ğŸ“ˆ Explore emotion segments over time


ğŸ”§ Components
LSTM (256 units) â€” captures long-term emotion cues
GRU (128 â†’ 64 units) â€” efficient short-term pattern modeling
Multi-Head Attention â€” focuses on emotion-rich frames
BatchNorm + Dropout â€” stabilizes and regularizes
Dense + Softmax â€” final emotion classification

ğŸšï¸ Extracted Audio Features
MFCC
Delta & Delta-Delta MFCCs
Chroma
Mel-Spectrogram


These features capture the frequency, energy, harmony, and tone differences that distinguish emotions.

ğŸ”® Future Enhancements
ğŸ¤– Facial Emotion Recognition + Audio (Multimodal System)
ğŸ—£ï¸ Multilingual Emotion Support
ğŸ•¸ï¸ Web API (FastAPI/Flask)
ğŸ“± Mobile App Deployment
ğŸ” Transformer-based Speech Models (Wav2Vec2, HuBERT, Whisper Large-V3)

ğŸ“Œ Acknowledgements
TensorFlow / Keras
Librosa
OpenAI Whisper
Streamlit
Scikit-learn
```
